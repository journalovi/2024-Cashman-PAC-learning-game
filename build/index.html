<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, minimum-scale=1.0"
    />
    <title>pac_learning</title>
    <meta property="og:title" content="pac_learning" />
    <meta charset="utf-8" />
    <meta property="og:type" content="article" />

    <meta property="og:description" content="An illustration of PAC Learning and why ML models can fail in deployment." />
    <meta property="description" content="An illustration of PAC Learning and why ML models can fail in deployment." />
    <link
      rel="stylesheet"
      href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css"
    />

    <link rel="stylesheet" href="static/idyll_styles.css" />
  </head>
  <body>
    <div id="idyll-mount"><div data-reactroot=""><div class="idyll-root"><div class=" idyll-text-container"></div><div class="article-header" style="background:#222222;color:#ffffff"><h1 class="hed">PAC Learning</h1><h2 class="dek">Or: Why We Should (and Shouldn&#x27;t) Trust Machine Learning</h2><div class="byline">By: <a>Dylan Cashman, Assistant Professor of Computer Science, Brandeis University</a></div><div class="idyll-pub-date">October 18, 2023</div></div><div class=" idyll-text-container"><div style="position:fixed" class="fixed"><div class="pac-game-container" parentcurrgamemsg="Welcome to the Four Germans Game!" gamestate="NA"><div class="MuiPaper-root MuiPaper-elevation1 MuiPaper-rounded"><div class="MuiPaper-root MuiPaper-elevation3 MuiPaper-rounded"><div class="pac-game-message">Welcome to the Four Germans Game!</div></div><div class="MuiGrid-root MuiGrid-container MuiGrid-spacing-xs-3"><div class="MuiGrid-root MuiGrid-item MuiGrid-grid-xs-1"></div><div class="MuiGrid-root MuiGrid-item MuiGrid-grid-xs-11"><div class="MuiGrid-root MuiGrid-container MuiGrid-spacing-xs-4"><div class="MuiGrid-root MuiGrid-item MuiGrid-grid-xs-12"><button class="MuiButtonBase-root MuiButton-root MuiButton-text" tabindex="0" type="button"><span class="MuiButton-label"><svg class="MuiSvgIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M17.65 6.35C16.2 4.9 14.21 4 12 4c-4.42 0-7.99 3.58-7.99 8s3.57 8 7.99 8c3.73 0 6.84-2.55 7.73-6h-2.08c-.82 2.33-3.04 4-5.65 4-3.31 0-6-2.69-6-6s2.69-6 6-6c1.66 0 3.14.69 4.22 1.78L13 11h7V4l-2.35 2.35z"></path></svg></span></button><button class="MuiButtonBase-root MuiButton-root MuiButton-text" tabindex="0" type="button"><span class="MuiButton-label"><svg class="MuiSvgIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M6 19h4V5H6v14zm8-14v14h4V5h-4z"></path></svg></span></button><button class="MuiButtonBase-root MuiButton-root MuiButton-text selectedButton" tabindex="0" type="button"><span class="MuiButton-label"><svg class="MuiSvgIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M8 5v14l11-7z"></path></svg></span></button><button class="MuiButtonBase-root MuiButton-root MuiButton-text" tabindex="0" type="button"><span class="MuiButton-label"><svg class="MuiSvgIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M6 18l8.5-6L6 6v12zM16 6v12h2V6h-2z"></path></svg></span></button><button class="MuiButtonBase-root MuiButton-root MuiButton-text" tabindex="0" type="button"><span class="MuiButton-label">Test!</span></button></div></div><div class="MuiGrid-root MuiGrid-item MuiGrid-grid-xs-12"><div style="width:100%"></div></div><div class="MuiGrid-root MuiGrid-container" space="2"><div class="MuiGrid-root MuiGrid-item MuiGrid-grid-xs-6"><div class="evaluation-statistics evaluation-statistics-training"><div class="MuiGrid-root MuiGrid-container" space="2"><div class="MuiGrid-root MuiGrid-item MuiGrid-grid-xs-12"><div class="evaluation-accuracy"><span class="accuracy-span">Total Training Error: <!-- -->0.00<!-- -->%</span></div></div><div class="MuiGrid-root MuiGrid-item MuiGrid-grid-xs-6"><div class="evaluation-cm-item true-positive"><span class="tooltip">TP %:   <span class="tooltiptext">The percent of the samples seen in training that are correctly enclosed by the dragged square.</span></span><span class="cm-value"> <!-- -->0.00<!-- -->%</span></div></div><div class="MuiGrid-root MuiGrid-item MuiGrid-grid-xs-6"><div class="evaluation-cm-item false-positive"><span class="tooltip">FP %:   <span class="tooltiptext">The percent of the samples seen in training that are in the dragged square, but shouldn&#x27;t be.</span></span><span class="cm-value"> <!-- -->0.00<!-- -->%</span></div></div><div class="MuiGrid-root MuiGrid-item MuiGrid-grid-xs-6"><div class="evaluation-cm-item false-negative"><span class="tooltip">FN %:   <span class="tooltiptext">The percent of the samples seen in training that are not in the dragged square, but should be.</span></span><span class="cm-value"> <!-- -->0.00<!-- -->%</span></div></div><div class="MuiGrid-root MuiGrid-item MuiGrid-grid-xs-6"><div class="evaluation-cm-item true-negative"><span class="tooltip">TN %:   <span class="tooltiptext">The percent of the samples seen in training that are correctly not in the dragged square.</span></span><span class="cm-value"> <!-- -->0.00<!-- -->%</span></div></div></div></div></div><div class="MuiGrid-root MuiGrid-item MuiGrid-grid-xs-6"><div class="evaluation-statistics evaluation-statistics-testing"><div class="MuiGrid-root MuiGrid-container" space="2"><div class="MuiGrid-root MuiGrid-item MuiGrid-grid-xs-12"><div class="evaluation-accuracy"><span class="accuracy-span">Total Testing Error: <!-- -->0.00<!-- -->%</span></div></div><div class="MuiGrid-root MuiGrid-item MuiGrid-grid-xs-6"><div class="evaluation-cm-item true-positive"><span class="tooltip">TP %:   <span class="tooltiptext">The percent of the full space that is true positives (the dark green area).</span></span><span class="cm-value"> <!-- -->0.00<!-- -->%</span></div></div><div class="MuiGrid-root MuiGrid-item MuiGrid-grid-xs-6"><div class="evaluation-cm-item false-positive"><span class="tooltip">FP %:   <span class="tooltiptext">The percent of the full space that is false positives (the gray area).</span></span><span class="cm-value"> <!-- -->0.00<!-- -->%</span></div></div><div class="MuiGrid-root MuiGrid-item MuiGrid-grid-xs-6"><div class="evaluation-cm-item false-negative"><span class="tooltip">FN %:   <span class="tooltiptext">The percent of the full space that is false negatives (the light green area).</span></span><span class="cm-value"> <!-- -->0.00<!-- -->%</span></div></div><div class="MuiGrid-root MuiGrid-item MuiGrid-grid-xs-6"><div class="evaluation-cm-item true-negative"><span class="tooltip">TN %:   <span class="tooltiptext">The percent of the full space that is true negatives (the area out of either squar).</span></span><span class="cm-value"> <!-- -->0.00<!-- -->%</span></div></div></div></div></div></div></div></div></div></div></div><h2 id="abstract">Abstract</h2><p>In this interactive article, we present an interactive game that represents the types of tasks solved by machine learning algorithms.  We use this game to motivate the definition of Probably Approximately Correct (PAC) learning, illustrating a proof of PAC learnability for Empirical Risk Minimization (ERM).  Then, we show three types of vulnerabilities of ERM that often occur in applied machine learning - domain mismatch, dependencies in data, and an incorrect model class.  We conclude by arguing for the need for visualization to identify these issues in a modeling dataset.</p><h2 id="introduction">Introduction</h2><p><div class="game-changer" longtext="Welcome to the Four Germans Game!" shorttext="Try it out!" gamestate="beginning" progressvar="0" parentcurrgamestate="NA" parentcurrgamemsg="Welcome to the Four Germans Game!" scrolloffset="300"><button class="MuiButtonBase-root MuiFab-root MuiFab-sizeMedium MuiFab-extended MuiFab-primary" tabindex="0" type="button"><span class="MuiFab-label"><svg class="MuiSvgIcon-root rotate0 MuiSvgIcon-colorPrimary" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M12 2L4.5 20.29l.71.71L12 18l6.79 3 .71-.71z"></path></svg>Try it out!</span></button><button class="MuiButtonBase-root MuiIconButton-root" tabindex="0" type="button" title="As you scroll through this page, each time you see a button like this one, the game will change to match where you are in the text.  After that, to go back to any previous state, either click on the buttons, or refresh the page."><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm1 17h-2v-2h2v2zm2.07-7.75l-.9.92C13.45 12.9 13 13.5 13 15h-2v-.5c0-1.1.45-2.1 1.17-2.83l1.24-1.26c.37-.36.59-.86.59-1.41 0-1.1-.9-2-2-2s-2 .9-2 2H8c0-2.21 1.79-4 4-4s4 1.79 4 4c0 .88-.36 1.68-.93 2.25z"></path></svg></span></button></div>
Let’s play a game.  Your task is to define the concept of “men of medium height and weight”, and every second or so, I will give you examples.  A green dot means that example is part of the concept, and a red dot means it is not part of the concept.  Try the game out a few times, hitting the TEST! button when you’re satisfied with your guess, and then using the <svg class="MuiSvgIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M17.65 6.35C16.2 4.9 14.21 4 12 4c-4.42 0-7.99 3.58-7.99 8s3.57 8 7.99 8c3.73 0 6.84-2.55 7.73-6h-2.08c-.82 2.33-3.04 4-5.65 4-3.31 0-6-2.69-6-6s2.69-6 6-6c1.66 0 3.14.69 4.22 1.78L13 11h7V4l-2.35 2.35z"></path></svg> button to start a new game.</p><p>I first heard of this game from Tufts University professor emeritus Anselm Blumer, who along with his coauthors Ehrenfeucht, Haussler, and Warmuth used this setting to demonstrate the concept of learnability <a text="in their article in 1989" url="https://dl.acm.org/citation.cfm?id=76371" href="https://dl.acm.org/citation.cfm?id=76371">in their article in 1989</a>.  That paper became known as the “Four Germans Paper”, and in honor of their work, I call this game the <em>Four Germans Game</em>.  This game is a useful abstraction that can help illustrate the task that machine learning algorithms have to solve.  Machine learning models see some training data generated by an underlying phenomenon (i.e. the hidden box), and based on that limited information must fit a model that approximates the underlying phenomonen.  A successful algorithm will pick a concept with low error on its training data, and will hopefully have similarly low error when it is tested (here error defined as the total area that is only inside one box, i.e. false positives (in our proposed concept, not in the ground truth concept), or false negatives (in ground truth concept, not in our proposed concept)).  A lot can go wrong, however, even in this simple game:  </p><ul><li>What if the training data is not representative of the actual phenomenon?</li><li>What if there is not enough training data?</li><li>What if the training data is noisy?</li><li>What if the wrong model is used (i.e. you guess a rectangular region, but the actual ground truth is a circle)?</li></ul><p>
This game can be used to illustrate many concepts of one of the underlying theories of machine learning: <a text="probably approximately correct (PAC) learning" url="https://en.wikipedia.org/wiki/Probably_approximately_correct_learning#:~:text=In%20computational%20learning%20theory%2C%20probably,in%201984%20by%20Leslie%20Valiant." href="https://en.wikipedia.org/wiki/Probably_approximately_correct_learning#:~:text=In%20computational%20learning%20theory%2C%20probably,in%201984%20by%20Leslie%20Valiant.">probably approximately correct (PAC) learning</a>, proposed by Leslie Valiant in 1984.  PAC learning is a framework to help understand why we are able to trust machine learning algorithms.  It also provides some insight into why machine learning models can make nonsensical and erroneous precictions.  In this article, we’ll illustrate the basic assumptions of most machine learning algorithms, and demonstrate why fully-automated approaches will never be able to be error-free if these assumptions are validated.</p><h3 id="why-this-is-important">Why this is important</h3><p>Machine Learning algorithms are becoming ubiquitous tools in the age of big data to aid in decision making and data analysis. 
They are actively being deployed in scenarios where their impact directly effects humans,including <a text="self-driving cars" url="https://arxiv.org/abs/1901.04407" href="https://arxiv.org/abs/1901.04407">self-driving cars</a>, <a text="patient triaging software at hospitals" url="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0205836" href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0205836">patient triaging software at hospitals</a>, and even <a text="crime prediction software" url="https://link.springer.com/chapter/10.1007/978-3-030-14680-1_40" href="https://link.springer.com/chapter/10.1007/978-3-030-14680-1_40">crime prediction software</a>, a la <a text="Minority Report" url="https://www.imdb.com/title/tt0181689/" href="https://www.imdb.com/title/tt0181689/">Minority Report</a>.  Automation from deep learning models such as <a text="facial recognition technology" url="https://www.gao.gov/products/gao-22-106100" href="https://www.gao.gov/products/gao-22-106100">facial recognition technology</a>, <a text="image and scene classification" url="https://www.sciencedirect.com/science/article/pii/S0386111219301566" href="https://www.sciencedirect.com/science/article/pii/S0386111219301566">image and scene classification</a>, and <a text="text generation through large language models" url="https://openai.com/blog/chatgpt" href="https://openai.com/blog/chatgpt">text generation through large language models</a> are rapidly eclipsing levels of human competency and changing the way we live our lives.</p><p>When they are successful, these models can result in new industries and capabilities that were science fiction only years ago.  But when they fail, they can have drastic consequences for the humans effected by them.  Consider a scenario where making a mistake can have drastic consequences: for example, a model might be used to prescribe a medication that has drastic side effects if applied to the wrong patient.  Would you feel comfortable using a machine learning model to decide on that medication?  
When we deploy machine learning algorithms out in the wild, how do we <em>know</em> that they will work at their intended goal?  How do we know that the probability of their error is small <em>enough</em>?</p><p>The answer, of course, is that we don’t know for sure the models will work as intended.  
We train models on an available dataset, and then hope that the model’s performance on a portion of that dataset is representative 
on however that model will be used out in the wild.  This can fail spectacularly, though: <a text="A recent project called called Gender Shades by Joy Buolamwini et al." url="http://gendershades.org/" href="http://gendershades.org/">A recent project called called Gender Shades by Joy Buolamwini et al.</a> at the Massachusetts Institute of Technology discovered that commercial gender classification systems have less than one percent error on lighter-skinned males, and up to 35% error on darker-skinned females, presumably because they were trained on datasets that were biased towards light-skinned males.  Because the training sets were imbalanced and not representative of the general population, the tool performed much worse in practice than we can imagine its creators assumed it would perform.</p><p>It is important for the general public to understand that machine learning algorithms make mistakes.  And it is <em>crucial</em> for decision makers to understand <em>why</em> machine learning models can perform worse when deployed in the real world, so that they can make informed decisions about deploying these models in high-risk environments.  In hindsight, it might seem obvious that the training data of facial recognition algorithms could lead to a bias in a deployed system.  PAC learning, however, will provide us foresight to anticipate these types of risks before building a model.  And this article will illustrate what that means. </p><h2 id="four-germans-game:-find-my-rectangle">Four Germans Game: Find My Rectangle</h2><div class="game-changer" longtext="Now, try to guess with no labeled points" shorttext="No Free Lunch" gamestate="no_free_lunch" progressvar="0" parentcurrgamestate="NA" parentcurrgamemsg="Welcome to the Four Germans Game!" scrolloffset="300"><button class="MuiButtonBase-root MuiFab-root MuiFab-sizeMedium MuiFab-extended MuiFab-primary" tabindex="0" type="button"><span class="MuiFab-label"><svg class="MuiSvgIcon-root rotate0 MuiSvgIcon-colorPrimary" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M12 2L4.5 20.29l.71.71L12 18l6.79 3 .71-.71z"></path></svg>No Free Lunch</span></button></div><p>Let’s return to the game, and consider some machine learning concepts.  First, try to play the game again (in the smaller version to the right here), but without getting any training data.  </p><p>...</p><p>Not very fun, or interesting, right?</p><p>This is an illustration of there being <a text="No Free Lunch theorems for optimization" url="https://ieeexplore.ieee.org/document/585893" href="https://ieeexplore.ieee.org/document/585893">No Free Lunch theorems for optimization</a>.  In the absence of any information, we can do no better than random chance, and thus we can make no statements about how “correct” we are.  You might have 10% error, you might have 90% error, you might fit the rectangle perfectly and have 0 error.  </p><div class=""><div class="section-divider"><svg class="MuiSvgIcon-root section-divider-arrow MuiSvgIcon-fontSizeLarge" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M20 12l-1.41-1.41L13 16.17V4h-2v12.17l-5.58-5.59L4 12l8 8 8-8z"></path></svg></div><hr/></div><div class="game-changer" longtext="How would you guess with 10 labeled points?" shorttext="A Little Lunch, as a Treat" gamestate="ten_samples" progressvar="0" parentcurrgamestate="NA" parentcurrgamemsg="Welcome to the Four Germans Game!" scrolloffset="300"><button class="MuiButtonBase-root MuiFab-root MuiFab-sizeMedium MuiFab-extended MuiFab-primary" tabindex="0" type="button"><span class="MuiFab-label"><svg class="MuiSvgIcon-root rotate0 MuiSvgIcon-colorPrimary" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M12 2L4.5 20.29l.71.71L12 18l6.79 3 .71-.71z"></path></svg>A Little Lunch, as a Treat</span></button></div><p>However, suppose we are given some information.  Here, we are given 10 points, with 5 inside the target rectangle and 5 outside.  If you guess a rectangle, can you make any guarantees about how close you are to the ground truth?</p><p>Consider the various strategies that you might use to minimize your error.  Do you tightly wrap the green examples, or do you leave some space around them to allow for data you haven’t seen yet?  Which strategy generally works better?  Which strategy works better in the unlucky case, where the sampled data doesn’t provide much information due to bad luck?</p><p>Up to this point, you have been playing the machine learning algorithm.  You have all the magnificent faculties of the human mind, letting you change and adapt your strategy as you encounter new examples.  But machine learning algorithms tend to be much simpler, and typically more stuck-in-their ways, because they have to be well-defined.  </p><div class=""><div class="section-divider"><svg class="MuiSvgIcon-root section-divider-arrow MuiSvgIcon-fontSizeLarge" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M20 12l-1.41-1.41L13 16.17V4h-2v12.17l-5.58-5.59L4 12l8 8 8-8z"></path></svg></div><hr/></div><p>We present three simple machine learning algorithms to solve this problem.</p><div class="game-changer" longtext="This algorithm picks the tightest rectangle." shorttext="Tightest Fit" gamestate="tightest_fit" progressvar="0" parentcurrgamestate="NA" parentcurrgamemsg="Welcome to the Four Germans Game!" scrolloffset="300"><button class="MuiButtonBase-root MuiFab-root MuiFab-sizeMedium MuiFab-extended MuiFab-primary" tabindex="0" type="button"><span class="MuiFab-label"><svg class="MuiSvgIcon-root rotate0 MuiSvgIcon-colorPrimary" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M12 2L4.5 20.29l.71.71L12 18l6.79 3 .71-.71z"></path></svg>Tightest Fit</span></button></div><ol><li><strong>Tightest Fit</strong>:  In this algorithm, the tightest possible rectangle is chosen that still contains all of the positive examples seen so far.  This algorithm is the smallest possible region that is still consistent with the samples seen.  It is related to the principle of <a text="Risk Minimization" url="https://papers.nips.cc/paper/506-principles-of-risk-minimization-for-learning-theory" href="https://papers.nips.cc/paper/506-principles-of-risk-minimization-for-learning-theory">Risk Minimization</a>, which is a strategy applicable to many machine learning problems.  This will under-estimate the rectangular region.</li></ol><div class=""><div class="section-divider"><svg class="MuiSvgIcon-root section-divider-arrow MuiSvgIcon-fontSizeLarge" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M20 12l-1.41-1.41L13 16.17V4h-2v12.17l-5.58-5.59L4 12l8 8 8-8z"></path></svg></div><hr/></div><div class="game-changer" longtext="This algorithm picks the loosest rectangle" shorttext="Loosest Fit" gamestate="loosest_fit" progressvar="0" parentcurrgamestate="NA" parentcurrgamemsg="Welcome to the Four Germans Game!" scrolloffset="300"><button class="MuiButtonBase-root MuiFab-root MuiFab-sizeMedium MuiFab-extended MuiFab-primary" tabindex="0" type="button"><span class="MuiFab-label"><svg class="MuiSvgIcon-root rotate0 MuiSvgIcon-colorPrimary" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M12 2L4.5 20.29l.71.71L12 18l6.79 3 .71-.71z"></path></svg>Loosest Fit</span></button></div><ol><li><strong>Loosest Fit</strong>:  In contrast to the Tightest Fit algorithm, this algorithm chooses the loosest possible rectangle.  This rectangle will be internal to all negative examples, and will contain all positive examples.  This will over-estimate the rectangular region.</li></ol><div class=""><div class="section-divider"><svg class="MuiSvgIcon-root section-divider-arrow MuiSvgIcon-fontSizeLarge" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M20 12l-1.41-1.41L13 16.17V4h-2v12.17l-5.58-5.59L4 12l8 8 8-8z"></path></svg></div><hr/></div><div class="game-changer" longtext="This algorithm maximizes the margin between in and out" shorttext="Maximum Margin" gamestate="max_margin" progressvar="0" parentcurrgamestate="NA" parentcurrgamemsg="Welcome to the Four Germans Game!" scrolloffset="300"><button class="MuiButtonBase-root MuiFab-root MuiFab-sizeMedium MuiFab-extended MuiFab-primary" tabindex="0" type="button"><span class="MuiFab-label"><svg class="MuiSvgIcon-root rotate0 MuiSvgIcon-colorPrimary" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M12 2L4.5 20.29l.71.71L12 18l6.79 3 .71-.71z"></path></svg>Maximum Margin</span></button></div><ol><li><strong>Maximum Margin</strong>: This is a midpoint between the two previous algorithms.  It chooses a rectangle that is as far as possible from both the positive and negative examples, while containing all examples.</li></ol><p>
How do we determine if any of these algorithms are good?  Or reliable?  What are the properties and assumptions that are necessary to make these judgements?</p><div class=""><div class="section-divider"><svg class="MuiSvgIcon-root section-divider-arrow MuiSvgIcon-fontSizeLarge" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M20 12l-1.41-1.41L13 16.17V4h-2v12.17l-5.58-5.59L4 12l8 8 8-8z"></path></svg></div><hr/></div><h2 id="pac-learning">PAC-learning</h2><p>When we use classical statistical methods, such as logistic regression, to analyze data, we can <em>trust</em> the model works because formalisms such as the p value and confidence intervals give us bounds on the possible errors of the model.  The rectangle game proposed by Blumer et. al. that is featured in this post is designed to demonstrate how such formalisms can be defined for machine learning techniques.  </p><p>Computational Learning Theorists attempt to find theoretically sound definitions of concepts found in machine learning, 
such as training data, validation accuracy, and modelling.  These definitions are then used to prove bounds on various 
metrics like error and runtime.  The paradigm of <strong>probably approximately correct learning</strong>, or <strong>PAC learning</strong>, has the 
explicit goal of determining under what conditions a machine learning algorithm will most likely perform about as well as it does 
on a training set, when deployed in the wild.</p><p>We provide the following treatment based on chapter 1 from <a text="Kearns and Vazirani" url="https://mitpress.mit.edu/books/introduction-computational-learning-theory" href="https://mitpress.mit.edu/books/introduction-computational-learning-theory">Kearns and Vazirani</a>, and we direct the reader to that text for a more thorough discussion.  Loosely, a concept is PAC-learnable if there exists a machine learning algorithm such that, after viewing a sufficient amount of labeled samples, we can prove a bound on the error (let’s say that error is smaller than some epsilon <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">ϵ</span></span></span></span> </span></span>), assuming we aren’t too unlucky with which samples we receive (so, with probability at least <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mn>1</mn><mo>−</mo><mi>δ</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">(1 - \delta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mopen">(</span><span class="mord">1</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mord mathit" style="margin-right:0.03785em;">δ</span><span class="mclose">)</span></span></span></span> </span></span>).  This type of analysis mirrors analysis done using p values for logistic regression or mathematical definitions of limits, and allows us to bound error on machine learning models.   We define it formally below, then prove that our game is PAC-learnable.  The assumptions in the definition and its proof lead us to a better understanding of potential vulnerabilities of machine learning algorithms, which will be demonstrated below.</p><div class="game-changer" longtext="Pause For Definitions" shorttext="Pause The Game" gamestate="pause_definitions" progressvar="0" parentcurrgamestate="NA" parentcurrgamemsg="Welcome to the Four Germans Game!" scrolloffset="300"><button class="MuiButtonBase-root MuiFab-root MuiFab-sizeMedium MuiFab-extended MuiFab-primary" tabindex="0" type="button"><span class="MuiFab-label"><svg class="MuiSvgIcon-root rotate0 MuiSvgIcon-colorPrimary" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M12 2L4.5 20.29l.71.71L12 18l6.79 3 .71-.71z"></path></svg>Pause The Game</span></button></div></div><div class="fullWidth"><hr/><div class="callout"><div class="callout-definition"><h2 id="definition">Definition</h2><p>(<a text="Adapted from Wikipedia" url="https://en.wikipedia.org/wiki/Probably_approximately_correct_learning" href="https://en.wikipedia.org/wiki/Probably_approximately_correct_learning">Adapted from Wikipedia</a>)</p><p>Let <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.07847em;">X</span></span></span></span> </span></span> be a data space.  This could be as simple as our 2-D space, or it could be the space of all images or the space of all snippets of text.  A <em>concept</em> is a subset  <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi><mo>∈</mo><mi>X</mi></mrow><annotation encoding="application/x-tex">c \in X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.72243em;vertical-align:-0.0391em;"></span><span class="base"><span class="mord mathit">c</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mord mathit" style="margin-right:0.07847em;">X</span></span></span></span> </span></span> of the data space - in our case a rectangular region.  A <em>concept class</em> <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.07153em;">C</span></span></span></span> </span></span> is the collection of all concepts over our data space - in our case, all possible rectangular regions in the 2-D space.  </p><p>Let <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>E</mi><mi>X</mi><mo>(</mo><mi>c</mi><mo separator="true">,</mo><mi>D</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">EX(c, D)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.05764em;">E</span><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="mopen">(</span><span class="mord mathit">c</span><span class="mpunct">,</span><span class="mord rule" style="margin-right:0.16666666666666666em;"></span><span class="mord mathit" style="margin-right:0.02778em;">D</span><span class="mclose">)</span></span></span></span> </span></span> be a procedure that draws an example, <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">x</span></span></span></span> </span></span>, using a probability distribution <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.02778em;">D</span></span></span></span> </span></span> and gives the correct label <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">c(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord mathit">c</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span> </span></span>.  In our case, <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>E</mi><mi>X</mi><mo>(</mo><mi>c</mi><mo separator="true">,</mo><mi>D</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">EX(c, D)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.05764em;">E</span><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="mopen">(</span><span class="mord mathit">c</span><span class="mpunct">,</span><span class="mord rule" style="margin-right:0.16666666666666666em;"></span><span class="mord mathit" style="margin-right:0.02778em;">D</span><span class="mclose">)</span></span></span></span> </span></span> is the process that happens when you hit the <svg class="MuiSvgIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M8 5v14l11-7z"></path></svg> button.  It draws a random location in the 2-D space using <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.02778em;">D</span></span></span></span> </span></span>(the uniform distribution), then provides a label for that location as green (if it is part of the rectangular region we want to guess), or red (if it is outside that region).</p><p>An algorithm <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">A</span></span></span></span> </span></span> is a <strong>PAC learning algorithm</strong> for <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.07153em;">C</span></span></span></span> </span></span> (equivalently, the concept space <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.07153em;">C</span></span></span></span> </span></span> is <strong>PAC learnable</strong> if the following is true: For any <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">ϵ</span></span></span></span> </span></span> and <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>δ</mi></mrow><annotation encoding="application/x-tex">\delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.03785em;">δ</span></span></span></span> </span></span> that we choose between 0 and 1 (i.e. some small fraction of a number), there exists a finite sample size <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathit">p</span></span></span></span> </span></span> such that if <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>E</mi><mi>X</mi><mo>(</mo><mi>c</mi><mo separator="true">,</mo><mi>D</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">EX(c, D)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.05764em;">E</span><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="mopen">(</span><span class="mord mathit">c</span><span class="mpunct">,</span><span class="mord rule" style="margin-right:0.16666666666666666em;"></span><span class="mord mathit" style="margin-right:0.02778em;">D</span><span class="mclose">)</span></span></span></span> </span></span> draws and labels <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathit">p</span></span></span></span> </span></span> examples, the algorithm <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">A</span></span></span></span> </span></span> will output a hypothesis concept, <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>h</mi><mo>∈</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">h \in C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.73354em;vertical-align:-0.0391em;"></span><span class="base"><span class="mord mathit">h</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mord mathit" style="margin-right:0.07153em;">C</span></span></span></span> </span></span>, that will have less than <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">ϵ</span></span></span></span> </span></span> error, with probability <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo>−</mo><mi>δ</mi></mrow><annotation encoding="application/x-tex">1-\delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="base"><span class="mord">1</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mord mathit" style="margin-right:0.03785em;">δ</span></span></span></span> </span></span>.</p></div><p>In other words, a type of concept (which could be rectangular regions on a plane, or the set of all pictures of cats and dogs within a group of images - the thing we are trying to see in the data space) is considered learnable if a learning algorithm exists that can usually reach a level of guaranteed performance if it sees enough training data.  In the definition, choosing <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">ϵ</span></span></span></span> </span></span> chooses how “good” an algorithm we want.  If <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi><mo>=</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>0</mn><mn>5</mn></mrow><annotation encoding="application/x-tex">\epsilon = 0.05</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">ϵ</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">0</span><span class="mord">5</span></span></span></span> </span></span>, then we are saying a “good” algorithm will have less than 5% error, however we want to define error.  The <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>δ</mi></mrow><annotation encoding="application/x-tex">\delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.03785em;">δ</span></span></span></span> </span></span> instead compensates for bad luck.  If <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>δ</mi><mo>=</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">\delta = 0.1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.03785em;">δ</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">1</span></span></span></span> </span></span>, then we are saying that we will be able to produce a “good” algorithm at least 90% of the time.  The other 10% of the time, we might be unable to produce a “good” enough algorithm because of getting unlucky with a sample size.  In general, the number of samples increases as the amount of error (<span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">ϵ</span></span></span></span> </span></span>) and bad luck (<span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>δ</mi></mrow><annotation encoding="application/x-tex">\delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.03785em;">δ</span></span></span></span> </span></span>) we tolerate goes down.  These two parameters in the definition explain the redundancy in the term: probably (<span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>δ</mi></mrow><annotation encoding="application/x-tex">\delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.03785em;">δ</span></span></span></span> </span></span>) approximately (<span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">ϵ</span></span></span></span> </span></span>) correct.</p><p>It’s useful to consider when a concept is <em>not</em> PAC learnable.  It would seem pretty universal that the more samples a learning algorithm sees, the more confident it would get about its predictions, and the better those predictions would be.  But in a lot of problems, you hit a plateau.  If you try to fit a very simple model, like a logistic regression classifier, to a very complicated concept, like classifying photos of dogs, you probably can’t guarantee any arbitrary level of performance no matter how many samples you see.</p><h3 id="why-is-this-important?">Why is this important?</h3><p>The most popular learning paradigm used in machine learning use cases is <a text="*Empirical Risk Minimization*" url="https://en.wikipedia.org/wiki/Empirical_risk_minimization" href="https://en.wikipedia.org/wiki/Empirical_risk_minimization">*Empirical Risk Minimization*</a>, which says that when we want to minimize error (and thus risk) when putting a machine learning model out in the world, we should first tune it to minimize error on the <em>empirical</em> data that we have, i.e. the training data.  By minimizing the <em>empirical risk</em>, we are hopefully minimizing the <em>actual risk</em> the model will try to avoid when it is used out in the world.  </p><p>This only makes sense, though, if there is some theoretical basis for why seeing labeled trainign data would guarantee performance on unseen data.  If a concept class wasn’t PAC learnable, then no matter how good our performance on a training set was, we wouldn’t be able to make any guarantees on an algorithm’s performance out in the world.</p><p>Understanding how to prove PAC learnability can help us see what stipulations about a machine learning use case need to hold in order to trust empirical risk minimization.  The definition of PAC learnability can also provide hints into how applied machine learning can go wrong.  </p><p>In this article, we will use our game to demonstrate first how a learning algorithm can be proven to be a PAC learning algorithm.  Then, we will break down different ways where PAC learning can break down due to issues with the data or the complexity of the concept that we are trying to model.</p></div></div><div class=" idyll-text-container"><div class="extraspace"><div class="section-divider"><svg class="MuiSvgIcon-root section-divider-arrow MuiSvgIcon-fontSizeLarge" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M20 12l-1.41-1.41L13 16.17V4h-2v12.17l-5.58-5.59L4 12l8 8 8-8z"></path></svg></div><hr/></div><div class="game-changer" longtext="This algorithm picks the tightest rectangle." shorttext="Tightest Fit" gamestate="pac_learning_1" progressvar="0" parentcurrgamestate="NA" parentcurrgamemsg="Welcome to the Four Germans Game!" scrolloffset="300"><button class="MuiButtonBase-root MuiFab-root MuiFab-sizeMedium MuiFab-extended MuiFab-primary" tabindex="0" type="button"><span class="MuiFab-label"><svg class="MuiSvgIcon-root rotate0 MuiSvgIcon-colorPrimary" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M12 2L4.5 20.29l.71.71L12 18l6.79 3 .71-.71z"></path></svg>Tightest Fit</span></button></div><p>We can use the PAC learning paradigm to analyze our algorithms.  In particular, we can look at the tightest fit algorithm, and try to bound its error, where error is defined as the probability that our algorithm’s chosen region will get the label incorrect.  We would like to prove that the error is less than <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">ϵ</span></span></span></span> </span></span>, with probability at least <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo>−</mo><mi>δ</mi></mrow><annotation encoding="application/x-tex">1-\delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="base"><span class="mord">1</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mord mathit" style="margin-right:0.03785em;">δ</span></span></span></span> </span></span>.  We do this by showing that, under the uniform distribution that gives us our labeled points, we have a pretty good chance of having low error.</p><div class=""><div class="section-divider"><svg class="MuiSvgIcon-root section-divider-arrow MuiSvgIcon-fontSizeLarge" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M20 12l-1.41-1.41L13 16.17V4h-2v12.17l-5.58-5.59L4 12l8 8 8-8z"></path></svg></div><hr/></div><div class="game-changer" longtext="The tightest-fit rectangle is always contained in the target rectangle." shorttext="Tightest Fit Errors" gamestate="pac_learning_2" progressvar="0" parentcurrgamestate="NA" parentcurrgamemsg="Welcome to the Four Germans Game!" scrolloffset="300"><button class="MuiButtonBase-root MuiFab-root MuiFab-sizeMedium MuiFab-extended MuiFab-primary" tabindex="0" type="button"><span class="MuiFab-label"><svg class="MuiSvgIcon-root rotate0 MuiSvgIcon-colorPrimary" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M12 2L4.5 20.29l.71.71L12 18l6.79 3 .71-.71z"></path></svg>Tightest Fit Errors</span></button></div><p>First, we note that the tightest-fit rectangle is always contained in the target rectangle.  We can express the total error region as four strips: the strips above, below, to the left, and to the right of the tightest-fit rectangle.  This is the only region where our algorithm will label incorrectly.  To proceed, we determine the probability that the size of each region is <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>≤</mo><mfrac><mrow><mi>ϵ</mi></mrow><mrow><mn>4</mn></mrow></mfrac></mrow><annotation encoding="application/x-tex">\leq \frac{\epsilon}{4}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.695392em;"></span><span class="strut bottom" style="height:1.040392em;vertical-align:-0.345em;"></span><span class="base"><span class="mrel">≤</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.695392em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span><span style="top:-3.15em;"><span class="pstrut" style="height:3em;"></span><span class="stretchy" style="height:0.2em;"><svg width='400em' height='0.2em' viewBox='0 0 400000 200' preserveAspectRatio='xMinYMin slice'><path d='M0 80H400000 v40H0z M0 80H400000 v40H0z'/></svg></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">ϵ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> </span></span>, so that the total error is less than <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">ϵ</span></span></span></span> </span></span>.  This will give us a relationship between <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">ϵ</span></span></span></span> </span></span> and <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>δ</mi></mrow><annotation encoding="application/x-tex">\delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.03785em;">δ</span></span></span></span> </span></span> in terms of the nubmer of samples drawn, <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.10903em;">M</span></span></span></span> </span></span>.</p><div class="game-changer" longtext="Consider just the error on the top strip, T&#x27;." shorttext="Error from Top Strip" gamestate="pac_learning_3" progressvar="0" parentcurrgamestate="NA" parentcurrgamemsg="Welcome to the Four Germans Game!" scrolloffset="300"><button class="MuiButtonBase-root MuiFab-root MuiFab-sizeMedium MuiFab-extended MuiFab-primary" tabindex="0" type="button"><span class="MuiFab-label"><svg class="MuiSvgIcon-root rotate0 MuiSvgIcon-colorPrimary" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M12 2L4.5 20.29l.71.71L12 18l6.79 3 .71-.71z"></path></svg>Error from Top Strip</span></button></div><p>Consider the top strip, which we’ll call T’.  Then, consider the strip T that starts from the top of the proposal region, and sweeps down in height until it has area <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mi>ϵ</mi></mrow><mrow><mn>4</mn></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\epsilon}{4}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.695392em;"></span><span class="strut bottom" style="height:1.040392em;vertical-align:-0.345em;"></span><span class="base"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.695392em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span><span style="top:-3.15em;"><span class="pstrut" style="height:3em;"></span><span class="stretchy" style="height:0.2em;"><svg width='400em' height='0.2em' viewBox='0 0 400000 200' preserveAspectRatio='xMinYMin slice'><path d='M0 80H400000 v40H0z M0 80H400000 v40H0z'/></svg></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">ϵ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> </span></span>.  <strong>If T’ is contained in T, then it has area less than <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mi>ϵ</mi></mrow><mrow><mn>4</mn></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\epsilon}{4}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.695392em;"></span><span class="strut bottom" style="height:1.040392em;vertical-align:-0.345em;"></span><span class="base"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.695392em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span><span style="top:-3.15em;"><span class="pstrut" style="height:3em;"></span><span class="stretchy" style="height:0.2em;"><svg width='400em' height='0.2em' viewBox='0 0 400000 200' preserveAspectRatio='xMinYMin slice'><path d='M0 80H400000 v40H0z M0 80H400000 v40H0z'/></svg></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">ϵ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> </span></span>, and we are done</strong>.  By our algorithm, we know that our T’ can only be greater in area than T if we haven’t seen any samples in T, since if we had, then our proposal region would contain that point.  The probability that M independent draws from the distribution (i.e. new points that we see) all miss the region T (and thus that the area is less than <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mi>ϵ</mi></mrow><mrow><mn>4</mn></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\epsilon}{4}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.695392em;"></span><span class="strut bottom" style="height:1.040392em;vertical-align:-0.345em;"></span><span class="base"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.695392em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span><span style="top:-3.15em;"><span class="pstrut" style="height:3em;"></span><span class="stretchy" style="height:0.2em;"><svg width='400em' height='0.2em' viewBox='0 0 400000 200' preserveAspectRatio='xMinYMin slice'><path d='M0 80H400000 v40H0z M0 80H400000 v40H0z'/></svg></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">ϵ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> </span></span>) is exactly <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mn>1</mn><mo>−</mo><mfrac><mrow><mi>ϵ</mi></mrow><mrow><mn>4</mn></mrow></mfrac><msup><mo>)</mo><mi>M</mi></msup></mrow><annotation encoding="application/x-tex">(1 - \frac{\epsilon}{4})^M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8413309999999999em;"></span><span class="strut bottom" style="height:1.186331em;vertical-align:-0.345em;"></span><span class="base"><span class="mopen">(</span><span class="mord">1</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.695392em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span><span style="top:-3.15em;"><span class="pstrut" style="height:3em;"></span><span class="stretchy" style="height:0.2em;"><svg width='400em' height='0.2em' viewBox='0 0 400000 200' preserveAspectRatio='xMinYMin slice'><path d='M0 80H400000 v40H0z M0 80H400000 v40H0z'/></svg></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">ϵ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight" style="margin-right:0.10903em;">M</span></span></span></span></span></span></span></span></span></span></span> </span></span>.  </p><div class="game-changer" longtext="Total error from four strips is less than epsilon." shorttext="Error from All Strips" gamestate="pac_learning_4" progressvar="0" parentcurrgamestate="NA" parentcurrgamemsg="Welcome to the Four Germans Game!" scrolloffset="300"><button class="MuiButtonBase-root MuiFab-root MuiFab-sizeMedium MuiFab-extended MuiFab-primary" tabindex="0" type="button"><span class="MuiFab-label"><svg class="MuiSvgIcon-root rotate0 MuiSvgIcon-colorPrimary" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M12 2L4.5 20.29l.71.71L12 18l6.79 3 .71-.71z"></path></svg>Error from All Strips</span></button></div><p>The same analysis holds for the other three regions, so by the union bound, the error is at most <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>4</mn><mo>(</mo><mn>1</mn><mo>−</mo><mfrac><mrow><mi>ϵ</mi></mrow><mrow><mn>4</mn></mrow></mfrac><msup><mo>)</mo><mi>M</mi></msup></mrow><annotation encoding="application/x-tex">4(1 - \frac{\epsilon}{4})^M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8413309999999999em;"></span><span class="strut bottom" style="height:1.186331em;vertical-align:-0.345em;"></span><span class="base"><span class="mord">4</span><span class="mopen">(</span><span class="mord">1</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.695392em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span><span style="top:-3.15em;"><span class="pstrut" style="height:3em;"></span><span class="stretchy" style="height:0.2em;"><svg width='400em' height='0.2em' viewBox='0 0 400000 200' preserveAspectRatio='xMinYMin slice'><path d='M0 80H400000 v40H0z M0 80H400000 v40H0z'/></svg></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">ϵ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight" style="margin-right:0.10903em;">M</span></span></span></span></span></span></span></span></span></span></span> </span></span>.  Then, we choose M to satisfy <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>4</mn><mo>(</mo><mn>1</mn><mo>−</mo><mfrac><mrow><mi>ϵ</mi></mrow><mrow><mn>4</mn></mrow></mfrac><msup><mo>)</mo><mi>M</mi></msup><mo>≤</mo><mi>δ</mi></mrow><annotation encoding="application/x-tex">4(1 - \frac{\epsilon}{4})^M \leq \delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8413309999999999em;"></span><span class="strut bottom" style="height:1.186331em;vertical-align:-0.345em;"></span><span class="base"><span class="mord">4</span><span class="mopen">(</span><span class="mord">1</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.695392em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span><span style="top:-3.15em;"><span class="pstrut" style="height:3em;"></span><span class="stretchy" style="height:0.2em;"><svg width='400em' height='0.2em' viewBox='0 0 400000 200' preserveAspectRatio='xMinYMin slice'><path d='M0 80H400000 v40H0z M0 80H400000 v40H0z'/></svg></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">ϵ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight" style="margin-right:0.10903em;">M</span></span></span></span></span></span></span></span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mord mathit" style="margin-right:0.03785em;">δ</span></span></span></span> </span></span>, i.e. the probability that the area is <strong>not</strong> bounded by <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">ϵ</span></span></span></span> </span></span> is smaller than <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>δ</mi></mrow><annotation encoding="application/x-tex">\delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.03785em;">δ</span></span></span></span> </span></span>, so the probability that it <strong>is</strong> bounded by <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">ϵ</span></span></span></span> </span></span> is greater than <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo>−</mo><mi>δ</mi></mrow><annotation encoding="application/x-tex">1 - \delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="base"><span class="mord">1</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mord mathit" style="margin-right:0.03785em;">δ</span></span></span></span> </span></span>.  </p><p>The final step of the proof is to determine how many samples we’d need - solving for M.  Using the inequality <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mn>1</mn><mo>−</mo><mi>x</mi><mo>)</mo><mo>≤</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow><annotation encoding="application/x-tex">(1 - x) \leq e^{-x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.771331em;"></span><span class="strut bottom" style="height:1.021331em;vertical-align:-0.25em;"></span><span class="base"><span class="mopen">(</span><span class="mord">1</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathit">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.771331em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathit mtight">x</span></span></span></span></span></span></span></span></span></span></span></span> </span></span>, we can simplify the bound to <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>4</mn><msup><mi>e</mi><mrow><mi>ϵ</mi><mi>M</mi><mi mathvariant="normal">/</mi><mn>4</mn></mrow></msup><mo>≤</mo><mi>δ</mi></mrow><annotation encoding="application/x-tex">4e^{\epsilon M / 4} \leq \delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8879999999999999em;"></span><span class="strut bottom" style="height:1.0239699999999998em;vertical-align:-0.13597em;"></span><span class="base"><span class="mord">4</span><span class="mord"><span class="mord mathit">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">ϵ</span><span class="mord mathit mtight" style="margin-right:0.10903em;">M</span><span class="mord mtight">/</span><span class="mord mtight">4</span></span></span></span></span></span></span></span></span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mord mathit" style="margin-right:0.03785em;">δ</span></span></span></span> </span></span>, and solve for <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi><mo>≥</mo><mo>(</mo><mn>4</mn><mi mathvariant="normal">/</mi><mi>ϵ</mi><mo>)</mo><mi>l</mi><mi>n</mi><mo>(</mo><mn>4</mn><mi mathvariant="normal">/</mi><mi>δ</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">M \geq (4/\epsilon)ln(4/\delta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.10903em;">M</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≥</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mopen">(</span><span class="mord">4</span><span class="mord">/</span><span class="mord mathit">ϵ</span><span class="mclose">)</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mord mathit">n</span><span class="mopen">(</span><span class="mord">4</span><span class="mord">/</span><span class="mord mathit" style="margin-right:0.03785em;">δ</span><span class="mclose">)</span></span></span></span> </span></span>.  This is a very common strategy for establishing bounds in these types of proofs.  The key takeaway is that M is not infinity.  Since M is a real, finite number, we’ve demonstrated that <strong>the problem is PAC-learnable</strong>, i.e. we have bounded the error in terms of the number of samples seen.</p><div class=""><div class="section-divider"><svg class="MuiSvgIcon-root section-divider-arrow MuiSvgIcon-fontSizeLarge" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M20 12l-1.41-1.41L13 16.17V4h-2v12.17l-5.58-5.59L4 12l8 8 8-8z"></path></svg></div><hr/></div><h2 id="assuming-the-worst">Assuming the Worst</h2><p>As the old saying goes: Don’t Assume.  It makes an ass out of you and me.  And our proof involved many assumptions that may not be so realistic, if we imagine proving something similar for a more applied case.</p><p>While the proof contains some complicated logic, and our conclusion can be hard to interpret, it was actually a significant result.  For this very simple problem (much simpler than the types of problems machine learning is being used to solve in the real world), we were able to show that the error on unseen examples would be bounded.  </p><p>But not so fast - there’s a major issue that we didn’t mention.  The proof we went through, as well as the PAC-learning paradigm in general, relies on several assumptions.</p><h3 id="independent-and-identically-distributed-data">Independent and Identically Distributed Data</h3><div class="game-changer" longtext="What if the data is not independent and identically distributed?" shorttext="Not IID data" gamestate="iid" progressvar="0" parentcurrgamestate="NA" parentcurrgamemsg="Welcome to the Four Germans Game!" scrolloffset="300"><button class="MuiButtonBase-root MuiFab-root MuiFab-sizeMedium MuiFab-extended MuiFab-primary" tabindex="0" type="button"><span class="MuiFab-label"><svg class="MuiSvgIcon-root rotate0 MuiSvgIcon-colorPrimary" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M12 2L4.5 20.29l.71.71L12 18l6.79 3 .71-.71z"></path></svg>Not IID data</span></button></div><p>That the sampled data in independent and identically distributed (typically referred to as i.i.d).  In real world data, this may not be the case; if the algorithm is being trained on streamed data, it typically cannot be considered independent draws.  This assumption was used in our probability bounds.</p><p>In this version of the game, the i.i.d. assumption is violated by placing some linear drift on the data.  As we get new points, they move linearly in a certain direction.</p><div class=""><div class="section-divider"><svg class="MuiSvgIcon-root section-divider-arrow MuiSvgIcon-fontSizeLarge" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M20 12l-1.41-1.41L13 16.17V4h-2v12.17l-5.58-5.59L4 12l8 8 8-8z"></path></svg></div><hr/></div><h3 id="training-testing-mismatch">Training-Testing Mismatch</h3><p><div class="game-changer" longtext="Here, the training data comes from a different distribution than the testing." shorttext="Train Test Mismatch" gamestate="train_test_mismatch" progressvar="0" parentcurrgamestate="NA" parentcurrgamemsg="Welcome to the Four Germans Game!" scrolloffset="300"><button class="MuiButtonBase-root MuiFab-root MuiFab-sizeMedium MuiFab-extended MuiFab-primary" tabindex="0" type="button"><span class="MuiFab-label"><svg class="MuiSvgIcon-root rotate0 MuiSvgIcon-colorPrimary" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M12 2L4.5 20.29l.71.71L12 18l6.79 3 .71-.71z"></path></svg>Train Test Mismatch</span></button></div>
We assume (or at least count on) that the data that we test our algorithm on is from the same distribution that we train our algorithm on.  This assumption is frequently incorrect in practice, and can result in unexpected behavior by the machine learning algorithm.  Consider a computer vision algorithm for a self-driving car that only trained in the Pacific Northwestern United States, and then was deployed in a desert climate; the behavior of the computer vision algorithm in training might have no relationship with its behavior in testing.  This would correspond to the testing samples being labeled in a different region than during training.</p><p>In this version of the game, the training distribution and the testing distribution are independent of one another.  This means that the true labels (green points) that you get in training may not be representative of where the true labels would appear in the testing phase.  It makes it impossible to guarantee any performance: it’s back to the No Free Lunch situation described earlier.</p><div class=""><div class="section-divider"><svg class="MuiSvgIcon-root section-divider-arrow MuiSvgIcon-fontSizeLarge" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M20 12l-1.41-1.41L13 16.17V4h-2v12.17l-5.58-5.59L4 12l8 8 8-8z"></path></svg></div><hr/></div><h3 id="incorrect-model-class">Incorrect Model Class</h3><div class="game-changer" longtext="What if our Model Class is Wrong?" shorttext="Ellipses, not Rectangles" gamestate="incorrect_model_class" progressvar="0" parentcurrgamestate="NA" parentcurrgamemsg="Welcome to the Four Germans Game!" scrolloffset="300"><button class="MuiButtonBase-root MuiFab-root MuiFab-sizeMedium MuiFab-extended MuiFab-primary" tabindex="0" type="button"><span class="MuiFab-label"><svg class="MuiSvgIcon-root rotate0 MuiSvgIcon-colorPrimary" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M12 2L4.5 20.29l.71.71L12 18l6.79 3 .71-.71z"></path></svg>Ellipses, not Rectangles</span></button></div><p>Our proof assumed that we already knew that the type of region we were looking for was a rectangle.  But in practice, we rarely know what kind of machine learning model will match the phenomenon being modeled in the data.  Suppose, instead of a rectangular region, the region we were looking for turned out to be an ellipse, or a parallellogram, or even an arbitrary, amorphous disconnected region.  A geometric proof would then be much harder, or even impossible.</p><p>In this version of the game, the actual region we are trying to guess is an ellipse, but we can only choose a rectangle.  Play it a few times and notice that no matter how good your guess is, you will always have some nonzero lower bound on your testing error.</p><div class=""><div class="section-divider"><svg class="MuiSvgIcon-root section-divider-arrow MuiSvgIcon-fontSizeLarge" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M20 12l-1.41-1.41L13 16.17V4h-2v12.17l-5.58-5.59L4 12l8 8 8-8z"></path></svg></div><hr/></div><p>There are other considerations for failure scenarios for machine learning algorithms.  Varshney presented <a text="an interesting treatment" url="https://www.liebertpub.com/doi/full/10.1089/big.2016.0051" href="https://www.liebertpub.com/doi/full/10.1089/big.2016.0051">an interesting treatment</a> in 2017.  In practice, there is active research into proving PAC learnability for more complex learning spaces than in this article (<a text="PAC Learnability of Node Functions in Networked Dynamical Systems by Adiga et al. in ICML from 2019" url="https://proceedings.mlr.press/v97/adiga19a" href="https://proceedings.mlr.press/v97/adiga19a">PAC Learnability of Node Functions in Networked Dynamical Systems by Adiga et al. in ICML from 2019</a>, or  <a text="A Theory of PAC Learnability under Transformation Invariances by Shao et al in NeurIPS from 2022" url="https://proceedings.neurips.cc/paper_files/paper/2022/hash/5a829e299ebc1c1615ddb09e98fb6ce8-Abstract-Conference.html" href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/5a829e299ebc1c1615ddb09e98fb6ce8-Abstract-Conference.html">A Theory of PAC Learnability under Transformation Invariances by Shao et al in NeurIPS from 2022</a>).  However, it is likely that PAC learnability wouldn’t hold in real world cases because we typically can’t <em>prove</em> (or <em>know</em>) that the assumptions are going to be held.  But the paradigm of PAC learnability illustrates what types of problems we have to look out for in applied machine learning.</p><h2 id="an-argument-for-visualization-in-applied-machine-learning">An Argument for Visualization in Applied Machine Learning</h2><p>The game played in this blog post was hopefully a relevant illustration of some of the topics introduced.  If it’s helpful, it may be because it is a visual explanation.  In general, visualizing the right properties of the data and the algorithm can help indicate to a data scientist whether any of the necessary assumptions are broken.  The data scientist can then address them by cleaning or preprocessing the data, or choosing a different machine learning algorithm.</p><p>In this blog post, we looked at an incredibly simple machine learning problem, and the algorithms we considered were easily explainable in a sentence or two.  Even for this simple problem, it was difficult to prove any limitations on error.  Machine learning is typically used to model much more complex problem domains, with much more complex data and intricate, high-dimensional processes.  In these cases, it is very unlikely that error bounds are proveable, and even if they are, it is unlikely that the assumptions are upheld.  </p><p>Instead, a human in the loop, armed with appropriate visualizations and analytical tools, can act as a safeguard against the most endemic cases.  This additional line of defense is more and more necessary as machine learning models are deployed in scenarios that directly affect humans.  More discussion on this topiccan be found in chapters 1 and 7 of my thesis, <a text="Bridging the Human-Machine Gap in Applied Machine Learning with Visual Analytics" url="https://www.cs.brandeis.edu/~dylan/public/docs/dylan_cashman_thesis_human_machine_gap.pdf" href="https://www.cs.brandeis.edu/~dylan/public/docs/dylan_cashman_thesis_human_machine_gap.pdf">Bridging the Human-Machine Gap in Applied Machine Learning with Visual Analytics</a>, which offers additional perspective on the role of visual analytics systems in empirical risk minimization.</p><div class=""><div class="section-divider"><svg class="MuiSvgIcon-root section-divider-arrow MuiSvgIcon-fontSizeLarge" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M20 12l-1.41-1.41L13 16.17V4h-2v12.17l-5.58-5.59L4 12l8 8 8-8z"></path></svg></div><hr/></div><h2 id="conclusion">Conclusion</h2><p>In this interactive article, we used the Four Germans Game to illustrate the concept of probably approximately correct (PAC) learning.  We then demonstrated how several assumptions are used in learning theory to build theoretical bounds on performance for machine learning models in the empirical risk minimization learning paradigm.  We presented some examples, using the game, of how these assumptions might be broken.  </p><h2 id="special-thanks">Special Thanks</h2><p>Thank you to Anselm Blumer and the reviewers from VisXAI.</p><h2 id="references">References</h2><ul><li>Blumer, Anselm, et al. “Learnability and the Vapnik-Chervonenkis dimension.” Journal of the ACM (JACM) <!-- -->3<!-- -->6<!-- -->.<!-- -->4 <!-- -->(<!-- -->1<!-- -->9<!-- -->8<!-- -->9<!-- -->): <!-- -->9<!-- -->2<!-- -->9<!-- -->-<!-- -->9<!-- -->6<!-- -->5<!-- -->.</li><li>Kearns, Michael J., and Umesh Vazirani. An introduction to computational learning theory. MIT press, <!-- -->1<!-- -->9<!-- -->9<!-- -->4<!-- -->.</li><li>Feng, Alice, et al. “The myth of the impartial machine.” The Parametric Press (<!-- -->2<!-- -->0<!-- -->1<!-- -->9<!-- -->).</li><li>Vapnik, Vladimir. “Principles of risk minimization for learning theory.” Advances in neural information processing systems <!-- -->4 <!-- -->(<!-- -->1<!-- -->9<!-- -->9<!-- -->1<!-- -->).</li><li>Varshney, Kush R., and Homa Alemzadeh. “On the safety of machine learning: Cyber-physical systems, decision sciences, and data products.” Big data <!-- -->5<!-- -->.<!-- -->3 <!-- -->(<!-- -->2<!-- -->0<!-- -->1<!-- -->7<!-- -->): <!-- -->2<!-- -->4<!-- -->6<!-- -->-<!-- -->2<!-- -->5<!-- -->5<!-- -->.</li><li>Valiant, Leslie. “Probably approximately correct: nature’s algorithms for learning and prospering in a complex world.” (<!-- -->2<!-- -->0<!-- -->1<!-- -->3<!-- -->).</li><li>Adiga, Abhijin, et al. “PAC learnability of node functions in networked dynamical systems.” International Conference on Machine Learning. PMLR, <!-- -->2<!-- -->0<!-- -->1<!-- -->9<!-- -->.</li><li>Shao, Han, Omar Montasser, and Avrim Blum. “A theory of pac learnability under transformation invariances.” Advances in Neural Information Processing Systems <!-- -->3<!-- -->5 <!-- -->(<!-- -->2<!-- -->0<!-- -->2<!-- -->2<!-- -->): <!-- -->1<!-- -->3<!-- -->9<!-- -->8<!-- -->9<!-- -->-<!-- -->1<!-- -->4<!-- -->0<!-- -->0<!-- -->1<!-- -->.</li><li>Badue, Claudine, et al. “Self-driving cars: A survey.” Expert Systems with Applications <!-- -->1<!-- -->6<!-- -->5 <!-- -->(<!-- -->2<!-- -->0<!-- -->2<!-- -->1<!-- -->): <!-- -->1<!-- -->1<!-- -->3<!-- -->8<!-- -->1<!-- -->6<!-- -->.</li><li>Kwon, Joon-myoung, et al. “Validation of deep-learning-based triage and acuity score using a large national dataset.” PloS one <!-- -->1<!-- -->3<!-- -->.<!-- -->1<!-- -->0 <!-- -->(<!-- -->2<!-- -->0<!-- -->1<!-- -->8<!-- -->): e<!-- -->0<!-- -->2<!-- -->0<!-- -->5<!-- -->8<!-- -->3<!-- -->6<!-- -->.</li><li>Wu, Shaobing, et al. “Crime prediction using data mining and machine learning.” The <!-- -->8<!-- -->th International Conference on Computer Engineering and Networks (CENet<!-- -->2<!-- -->0<!-- -->1<!-- -->8<!-- -->). Springer International Publishing, <!-- -->2<!-- -->0<!-- -->2<!-- -->0<!-- -->.</li><li>Fujiyoshi, Hironobu, Tsubasa Hirakawa, and Takayoshi Yamashita. “Deep learning-based image recognition for autonomous driving.” IATSS research <!-- -->4<!-- -->3<!-- -->.<!-- -->4 <!-- -->(<!-- -->2<!-- -->0<!-- -->1<!-- -->9<!-- -->): <!-- -->2<!-- -->4<!-- -->4<!-- -->-<!-- -->2<!-- -->5<!-- -->2<!-- -->.</li><li>Buolamwini, Joy, and Timnit Gebru. “Gender shades: Intersectional accuracy disparities in commercial gender classification.” Conference on fairness, accountability and transparency. PMLR, <!-- -->2<!-- -->0<!-- -->1<!-- -->8<!-- -->.</li><li>Wolpert, David H., and William G. Macready. “No free lunch theorems for optimization.” IEEE transactions on evolutionary computation <!-- -->1<!-- -->.<!-- -->1 <!-- -->(<!-- -->1<!-- -->9<!-- -->9<!-- -->7<!-- -->): <!-- -->6<!-- -->7<!-- -->-<!-- -->8<!-- -->2<!-- -->.</li></ul><h2 id="research-material-statements">Research Material Statements</h2><p>No datasets are used in this interactive article.  The data visualized within the game is either randomly generated or hardcoded in.  You can view the source code for this article on its <a text="Github repository" url="https://github.com/dylancashman/PAC-learning-game" href="https://github.com/dylancashman/PAC-learning-game">Github repository</a>.  Please see the software license below.</p><h2 id="about-the-author">About The Author</h2><p>Dylan Cashman is an assistant professor in the Michtom School of Computer Science at Brandeis University in Waltham, MA.  He previously worked as a Senior Expert in Data Science and Advanced Visual Analytics within the Data Science and Artificial Intelligence division of Novartis Pharmaceuticals in Cambridge, MA.  Dylan holds a PhD in Computer Science from Tufts University and an Sc. B in Mathematics from Brown University.  His research interests include the development and evaluation of visual affordances that improve usability of artificial intelligence models and data science processes.  His research has won best paper awards at the Eurovis conference, the Symposium on Visualization for Data Science (VDS), and the Workshop on Visual Analytics for Deep Learning at IEEEVIS.</p><h2 id="license">License</h2><p>This work is licensed under a <a text="Creative Commons Attribution-ShareAlike 4.0 International License" url="https://creativecommons.org/licenses/by-sa/4.0/" href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.</p><h2 id="conflict-of-interest">Conflict of Interest</h2><p>The author declares that there are no competing interests.</p></div></div></div></div>
    <script src="static/idyll_index.js"></script>
  </body>
</html>
